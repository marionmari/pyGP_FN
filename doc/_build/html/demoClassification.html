
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Classification Demonstration Example &mdash; Functional Gaussian Processes 1.0 documentation</title>
    
    <link rel="stylesheet" href="_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '',
        VERSION:     '1.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="top" title="Functional Gaussian Processes 1.0 documentation" href="index.html" />
    <link rel="up" title="Examples" href="Examples.html" />
    <link rel="next" title="Regression of Mauna Loa Data" href="demoMaunaLoa.html" />
    <link rel="prev" title="Regression Demonstration Example" href="demoRegression.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="demoMaunaLoa.html" title="Regression of Mauna Loa Data"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="demoRegression.html" title="Regression Demonstration Example"
             accesskey="P">previous</a> |</li>
        <li><a href="index.html">Functional Gaussian Processes 1.0 documentation</a> &raquo;</li>
          <li><a href="GettingStarted.html" >Getting Started</a> &raquo;</li>
          <li><a href="Examples.html" accesskey="U">Examples</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="classification-demonstration-example">
<h1>Classification Demonstration Example<a class="headerlink" href="#classification-demonstration-example" title="Permalink to this headline">Â¶</a></h1>
<p>This example recreates the classification example from the <a class="reference external" href="http://www.gaussianprocess.org/gpml/code/matlab/doc/">GPML</a> package.</p>
<p>The difference between regression and classification is not of a fundamental nature. We can still use a Gaussian process latent function
in essentially the same way, but unfortunately, the Gaussian likelihood function often used for regression is inappropriate for classification.
Since exact inference is only possible for Gaussian likelihood, we need an alternative, approximate, inference method.</p>
<p>Here, we will demonstrate binary classification, using two partially overlapping Gaussian sources of data in two dimensions.
First we import the data (so that it exactly matches the example in GPML):</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c">## LOAD data</span>
<span class="n">demoData</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">&#39;../../data/classification_data.npz&#39;</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">demoData</span><span class="p">[</span><span class="s">&#39;x&#39;</span><span class="p">]</span>            <span class="c"># training data</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">demoData</span><span class="p">[</span><span class="s">&#39;y&#39;</span><span class="p">]</span>            <span class="c"># training target</span>
<span class="n">xstar</span> <span class="o">=</span> <span class="n">demoData</span><span class="p">[</span><span class="s">&#39;xstar&#39;</span><span class="p">]</span>    <span class="c"># test data</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">xstar</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>           <span class="c"># number of test points</span>
</pre></div>
</div>
<p><span class="math">\(120\)</span> data points are generated from two Gaussians with different means and covariances. One Gaussian is isotropic and contains
<span class="math">\(2/3\)</span> of the data (blue), the other is highly correlated and contains <span class="math">\(1/3\)</span> of the points (red).
Note, that the labels for the targets are <span class="math">\(\pm 1\)</span> (and not <span class="math">\(0/1\)</span>).</p>
<p>In the plot, we superimpose the data points with the posterior equi-probability contour lines for the probability of the second class
given complete information about the generating mechanism.</p>
<div class="figure align-center">
<img alt="_images/demoC1.png" src="_images/demoC1.png" />
</div>
<p>We specify a Gaussian process model as follows: a constant mean function, <a class="reference internal" href="meanAPI.html#src.Core.means.meanConst" title="src.Core.means.meanConst"><tt class="xref py py-func docutils literal"><span class="pre">src.Core.means.meanConst()</span></tt></a> with initial parameter set to
<span class="math">\(0\)</span>, a squared exponential
with automatic relevance determination (ARD) covariance function <a class="reference internal" href="kernelAPI.html#src.Core.kernels.covSEard" title="src.Core.kernels.covSEard"><tt class="xref py py-func docutils literal"><span class="pre">src.Core.kernels.covSEard()</span></tt></a>. This covariance function has one
characteristic length-scale parameter for each dimension of the input space (<span class="math">\(2\)</span> total), and a signal magnitude parameter, for
a total of <span class="math">\(3\)</span> hyperparameters. ARD with separate length-scales for each input dimension is a very powerful tool to learn which
inputs are important for predictions: if length-scales are short, inputs are very important, and when they grow very long
(compared to the spread of the data), the corresponding inputs will be largely ignored. Both length-scales and the signal magnitude
are initialized to <span class="math">\(1\)</span> (and represented in the log space). Finally, the likelihood function <a class="reference internal" href="likelihoodAPI.html#src.Core.likelihoods.likErf" title="src.Core.likelihoods.likErf"><tt class="xref py py-func docutils literal"><span class="pre">src.Core.likelihoods.likErf()</span></tt></a>
has the shape of the error-function (or cumulative Gaussian), which doesn&#8217;t take any hyperparameters (so hyp.lik does not exist):</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c">## DEFINE parameterized mean and covariance functions</span>
<span class="n">meanfunc</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;means.meanConst&#39;</span><span class="p">]</span>
<span class="n">covfunc</span>  <span class="o">=</span> <span class="p">[</span><span class="s">&#39;kernels.covSEard&#39;</span><span class="p">]</span>
<span class="c">## DEFINE likelihood function used</span>
<span class="n">likfunc</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;likelihoods.likErf&#39;</span><span class="p">]</span>
<span class="c">## SPECIFY inference method</span>
<span class="n">inffunc</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;inferences.infLaplace&#39;</span><span class="p">]</span>

<span class="c">## SET (hyper)parameters</span>
<span class="n">hyp</span> <span class="o">=</span> <span class="n">hyperParameters</span><span class="p">()</span>
<span class="n">hyp</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">])</span>
<span class="n">hyp</span><span class="o">.</span><span class="n">cov</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span><span class="mf">0.</span><span class="p">,</span><span class="mf">0.</span><span class="p">])</span>
<span class="p">[</span><span class="n">hyp_opt</span><span class="p">,</span> <span class="n">fopt</span><span class="p">,</span> <span class="n">gopt</span><span class="p">,</span> <span class="n">funcCalls</span><span class="p">]</span> <span class="o">=</span> <span class="n">min_wrapper</span><span class="p">(</span><span class="n">hyp</span><span class="p">,</span><span class="n">gp</span><span class="p">,</span><span class="s">&#39;Minimize&#39;</span><span class="p">,</span><span class="n">inffunc</span><span class="p">,</span><span class="n">meanfunc</span><span class="p">,</span><span class="n">covfunc</span><span class="p">,</span><span class="n">likfunc</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="bp">None</span><span class="p">,</span><span class="bp">None</span><span class="p">,</span><span class="bp">True</span><span class="p">)</span>

<span class="n">hyp</span> <span class="o">=</span> <span class="n">hyp_opt</span>
</pre></div>
</div>
<p>We train the hyperparameters using minimize, to minimize the negative log marginal likelihood. We allow for <span class="math">\(100\)</span> function evaluations,
and specify that inference should be done with the Expectation Propagation (EP) inference method <a class="reference internal" href="inferenceAPI.html#src.Core.inferences.infEP" title="src.Core.inferences.infEP"><tt class="xref py py-func docutils literal"><span class="pre">src.Core.inferences.infEP()</span></tt></a>, and pass
the usual parameters.
Training is done using algorithm 3.5 and 5.2 from the <a class="reference external" href="http://www.gaussianprocess.org/gpml/code/matlab/doc/">GPML</a> book. When computing test probabilities, we call gp with additional test inputs,
and as the last argument a vector of targets for which the log probabilities lp should be computed. The fist four output arguments
of the function are mean and variance for the targets and corresponding latent variables respectively. The test set predictions are
computed using algorithm 3.6 from the GPML book. The contour plot for the predictive distribution is shown below. Note, that the predictive
probability is fairly close to the probabilities of the generating process in regions of high data density. Note also, that as you move
away from the data, the probability approaches <span class="math">\(1/3\)</span>, the overall class probability.</p>
<div class="figure align-center">
<img alt="_images/demoC2.png" src="_images/demoC2.png" />
</div>
<p>Examining the two ARD characteristic length-scale parameters after learning, you will find that they are fairly similar, reflecting the fact
that for this data set, both inputs are important.</p>
</div>
<div class="section" id="large-scale-classification">
<h1>Large scale classification<a class="headerlink" href="#large-scale-classification" title="Permalink to this headline">Â¶</a></h1>
<p>In case the number of training inputs <span class="math">\(x\)</span> exceeds a few hundred, approximate inference using <a class="reference internal" href="inferenceAPI.html#src.Core.inferences.infLaplace" title="src.Core.inferences.infLaplace"><tt class="xref py py-func docutils literal"><span class="pre">src.Core.inferences.infLaplace()</span></tt></a>,
<a class="reference internal" href="inferenceAPI.html#src.Core.inferences.infEP" title="src.Core.inferences.infEP"><tt class="xref py py-func docutils literal"><span class="pre">src.Core.inferences.infEP()</span></tt></a> and <tt class="xref py py-func docutils literal"><span class="pre">src.Core.inferences.infVB()</span></tt> takes too long. As in regression, we offer the FITC approximation
based on a low-rank plus diagonal approximation to the exact covariance to deal with these cases. The general idea is to use inducing points
<span class="math">\(u\)</span> and to base the computations on cross-covariances between training, test and inducing points only.</p>
<p>Using the FITC approximation is very simple, we just have to wrap the covariance function covfunc into <a class="reference internal" href="kernelAPI.html#src.Core.kernels.covFITC" title="src.Core.kernels.covFITC"><tt class="xref py py-func docutils literal"><span class="pre">src.Core.kernels.covFITC()</span></tt></a>
and call <tt class="xref py py-func docutils literal"><span class="pre">src.Core.gp()</span></tt> with the inference methods <a class="reference internal" href="inferenceAPI.html#src.Core.inferences.infFITC_Laplace" title="src.Core.inferences.infFITC_Laplace"><tt class="xref py py-func docutils literal"><span class="pre">src.Core.inferences.infFITC_Laplace()</span></tt></a> and <a class="reference internal" href="inferenceAPI.html#src.Core.inferences.infFITC_EP" title="src.Core.inferences.infFITC_EP"><tt class="xref py py-func docutils literal"><span class="pre">src.Core.inferences.infFITC_EP()</span></tt></a>
as demonstrated by the following lines of code:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c">## SPECIFY inducing points</span>
<span class="n">u1</span><span class="p">,</span><span class="n">u2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">u2</span><span class="p">,(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">u2</span><span class="o">.</span><span class="n">shape</span><span class="p">),)),</span><span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">u1</span><span class="p">,(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">u1</span><span class="o">.</span><span class="n">shape</span><span class="p">),))))</span>
<span class="k">del</span> <span class="n">u1</span><span class="p">,</span> <span class="n">u2</span>
<span class="n">nu</span> <span class="o">=</span> <span class="n">u</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c">## SPECIFY FITC covariance function</span>
<span class="n">covfuncF</span> <span class="o">=</span> <span class="p">[[</span><span class="s">&#39;kernels.covFITC&#39;</span><span class="p">],</span> <span class="n">covfunc</span><span class="p">,</span> <span class="n">u</span><span class="p">]</span>

<span class="c">## SPECIFY FITC inference method</span>
<span class="n">inffunc</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;inferences.infFITC_EP&#39;</span><span class="p">]</span>

<span class="c">## GET negative log marginal likelihood</span>
<span class="p">[</span><span class="n">nlml</span><span class="p">,</span><span class="n">dnlZ</span><span class="p">,</span><span class="n">post</span><span class="p">]</span> <span class="o">=</span> <span class="n">gp</span><span class="p">(</span><span class="n">hyp</span><span class="p">,</span> <span class="n">inffunc</span><span class="p">,</span> <span class="n">meanfunc</span><span class="p">,</span> <span class="n">covfuncF</span><span class="p">,</span> <span class="n">likfunc</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">True</span><span class="p">)</span>
<span class="k">print</span> <span class="s">&quot;nlml =&quot;</span><span class="p">,</span> <span class="n">nlml</span>

<span class="c">## TRAINING: OPTIMIZE hyperparameters</span>
<span class="p">[</span><span class="n">hyp_opt</span><span class="p">,</span> <span class="n">fopt</span><span class="p">,</span> <span class="n">gopt</span><span class="p">,</span> <span class="n">funcCalls</span><span class="p">]</span> <span class="o">=</span> <span class="n">min_wrapper</span><span class="p">(</span><span class="n">hyp</span><span class="p">,</span><span class="n">gp</span><span class="p">,</span><span class="s">&#39;Minimize&#39;</span><span class="p">,</span><span class="n">inffunc</span><span class="p">,</span><span class="n">meanfunc</span><span class="p">,</span><span class="n">covfuncF</span><span class="p">,</span><span class="n">likfunc</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="bp">None</span><span class="p">,</span><span class="bp">None</span><span class="p">,</span><span class="bp">True</span><span class="p">)</span>  <span class="c"># minimize by Carl$</span>
<span class="k">print</span> <span class="s">&#39;Optimal nlml =&#39;</span><span class="p">,</span> <span class="n">fopt</span>

<span class="c">## FITC PREDICTION</span>
<span class="p">[</span><span class="n">ymu</span><span class="p">,</span><span class="n">ys2</span><span class="p">,</span><span class="n">fmu</span><span class="p">,</span><span class="n">fs2</span><span class="p">,</span><span class="n">lp</span><span class="p">,</span><span class="n">post</span><span class="p">]</span> <span class="o">=</span> <span class="n">gp</span><span class="p">(</span><span class="n">hyp_opt</span><span class="p">,</span> <span class="n">inffunc</span><span class="p">,</span> <span class="n">meanfunc</span><span class="p">,</span> <span class="n">covfuncF</span><span class="p">,</span> <span class="n">likfunc</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">xstar</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">n</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span> <span class="p">)</span>
</pre></div>
</div>
<p>We define equispaced inducing points <span class="math">\(u\)</span> that are shown in the figure as black circles. Alternatively, a random subset of the training
points can be used as inducing points.</p>
<div class="figure align-center">
<img alt="_images/demoC3.png" src="_images/demoC3.png" />
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
  <h3><a href="index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Classification Demonstration Example</a></li>
<li><a class="reference internal" href="#large-scale-classification">Large scale classification</a></li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="demoRegression.html"
                        title="previous chapter">Regression Demonstration Example</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="demoMaunaLoa.html"
                        title="next chapter">Regression of Mauna Loa Data</a></p>
  <h3>This Page</h3>
  <ul class="this-page-menu">
    <li><a href="_sources/demoClassification.txt"
           rel="nofollow">Show Source</a></li>
  </ul>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="demoMaunaLoa.html" title="Regression of Mauna Loa Data"
             >next</a> |</li>
        <li class="right" >
          <a href="demoRegression.html" title="Regression Demonstration Example"
             >previous</a> |</li>
        <li><a href="index.html">Functional Gaussian Processes 1.0 documentation</a> &raquo;</li>
          <li><a href="GettingStarted.html" >Getting Started</a> &raquo;</li>
          <li><a href="Examples.html" >Examples</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2013, Marion Neumann, Daniel Marthaler, Shan Huang, Kristian Kersting.
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.1.3.
    </div>
  </body>
</html>